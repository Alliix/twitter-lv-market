{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0bdec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8eeff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = [\n",
    "  \"latvenergo\",\n",
    "  \"rimi\",\n",
    "  \"maxima\",\n",
    "  \"swedbank\",\n",
    "  \"lmt\",\n",
    "  \"tet\",\n",
    "  \"lg\",\n",
    "  \"tele2\",\n",
    "  \"airbaltic\",\n",
    "  \"olympic\",\n",
    "  \"seb\",\n",
    "  \"grindeks\",\n",
    "  \"citadele\",\n",
    "  \"bite\",\n",
    "  \"drogas\",\n",
    "  \"depo\",\n",
    "  \"circlek\",\n",
    "  \"lb\",\n",
    "  \"optibet\",\n",
    "  \"evolution\",\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ade0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>language</th>\n",
       "      <th>inReplyToStatusId</th>\n",
       "      <th>inReplyToUserId</th>\n",
       "      <th>inReplyToScreenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>...</th>\n",
       "      <th>retweetedId</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>label</th>\n",
       "      <th>message_lowercase</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>clean_message_no_punct</th>\n",
       "      <th>clean_message_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_freq_stopwords</th>\n",
       "      <th>clean_message_no_freq_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>1478404</td>\n",
       "      <td>925617390523732000</td>\n",
       "      <td>2017-11-01T08:55:57</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24855060</td>\n",
       "      <td>Dienas Bizness</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>vērtēti trīs potenciālie airbaltic investori U...</td>\n",
       "      <td>vērtēti trīs potenciālie airbaltic investori U...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>1486476</td>\n",
       "      <td>924948828318511100</td>\n",
       "      <td>2017-10-30T12:39:20</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44849531</td>\n",
       "      <td>RIGA I Airport</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien  vēlu vakarā  ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien , vēlu vakarā , svinīgu pasākumu atk...</td>\n",
       "      <td>vakardien vēlu vakarā svinīgu pasākumu atklāts...</td>\n",
       "      <td>vakardien vēlu vakarā svinīgu pasākumu tika at...</td>\n",
       "      <td>vakardien , vēlu vakarā , svinīgu pasākumu tik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Es ļoti ceru,ka potenciālie Air Baltic investo...</td>\n",
       "      <td>1488297</td>\n",
       "      <td>925794560420311000</td>\n",
       "      <td>2017-11-01T20:39:58</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218804015</td>\n",
       "      <td>Artis Pabriks</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>es ļoti ceru,ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru,ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru , potenciālie air baltic investor...</td>\n",
       "      <td>es ļoti ceru potenciālie air baltic investori ...</td>\n",
       "      <td>ļoti ceru potenciālie air baltic investori nav...</td>\n",
       "      <td>ļoti ceru , potenciālie air baltic investori n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ar kritisko domāšanu gan joprojām bēdīgi. Piln...</td>\n",
       "      <td>1489818</td>\n",
       "      <td>925045141018218500</td>\n",
       "      <td>2017-10-30T19:02:02</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21071644</td>\n",
       "      <td>Andris Rubīns</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi. piln...</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi. piln...</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi  piln...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi . pilns face...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi pilns facebo...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi pilns facebo...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi . pilns face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latvijas lidsabiedrība airBaltic sadarbībā ar ...</td>\n",
       "      <td>1490250</td>\n",
       "      <td>925006964135952400</td>\n",
       "      <td>2017-10-30T16:30:20</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131562098</td>\n",
       "      <td>Latviesi.com</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message       id  \\\n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...  1478404   \n",
       "1  Vakardien, vēlu vakarā, ar svinīgu pasākumu ti...  1486476   \n",
       "2  Es ļoti ceru,ka potenciālie Air Baltic investo...  1488297   \n",
       "3  Ar kritisko domāšanu gan joprojām bēdīgi. Piln...  1489818   \n",
       "4  Latvijas lidsabiedrība airBaltic sadarbībā ar ...  1490250   \n",
       "\n",
       "              tweetId            createdAt language  inReplyToStatusId  \\\n",
       "0  925617390523732000  2017-11-01T08:55:57       lv                NaN   \n",
       "1  924948828318511100  2017-10-30T12:39:20       lv                NaN   \n",
       "2  925794560420311000  2017-11-01T20:39:58       lv                NaN   \n",
       "3  925045141018218500  2017-10-30T19:02:02       lv                NaN   \n",
       "4  925006964135952400  2017-10-30T16:30:20       lv                NaN   \n",
       "\n",
       "   inReplyToUserId  inReplyToScreenName     userId        userName  ...  \\\n",
       "0              NaN                  NaN   24855060  Dienas Bizness  ...   \n",
       "1              NaN                  NaN   44849531  RIGA I Airport  ...   \n",
       "2              NaN                  NaN  218804015   Artis Pabriks  ...   \n",
       "3              NaN                  NaN   21071644   Andris Rubīns  ...   \n",
       "4              NaN                  NaN  131562098    Latviesi.com  ...   \n",
       "\n",
       "  retweetedId  retweetCount label  \\\n",
       "0         NaN             0     0   \n",
       "1         NaN             0     0   \n",
       "2         NaN             9     2   \n",
       "3         NaN             3     2   \n",
       "4         NaN             0     0   \n",
       "\n",
       "                                   message_lowercase  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien, vēlu vakarā, ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru,ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi. piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                                       clean_message  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien, vēlu vakarā, ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru,ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi. piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                              clean_message_no_punct  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien  vēlu vakarā  ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi  piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                clean_message_no_stopwords_from_list  \\\n",
       "0  vērtēti trīs potenciālie airbaltic investori U...   \n",
       "1  vakardien , vēlu vakarā , svinīgu pasākumu atk...   \n",
       "2  es ļoti ceru , potenciālie air baltic investor...   \n",
       "3  kritisko domāšanu joprojām bēdīgi . pilns face...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "       clean_message_no_punct_no_stopwords_from_list  \\\n",
       "0  vērtēti trīs potenciālie airbaltic investori U...   \n",
       "1  vakardien vēlu vakarā svinīgu pasākumu atklāts...   \n",
       "2  es ļoti ceru potenciālie air baltic investori ...   \n",
       "3  kritisko domāšanu joprojām bēdīgi pilns facebo...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "            clean_message_no_punct_no_freq_stopwords  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien vēlu vakarā svinīgu pasākumu tika at...   \n",
       "2  ļoti ceru potenciālie air baltic investori nav...   \n",
       "3  kritisko domāšanu joprojām bēdīgi pilns facebo...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "                     clean_message_no_freq_stopwords  \n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...  \n",
       "1  vakardien , vēlu vakarā , svinīgu pasākumu tik...  \n",
       "2  ļoti ceru , potenciālie air baltic investori n...  \n",
       "3  kritisko domāšanu joprojām bēdīgi . pilns face...  \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLabeledTweets = pd.read_csv('./tweets/allLabeledTweets.csv')\n",
    "allLabeledTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849c310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    253\n",
       "2    136\n",
       "1     87\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLabeledTweets[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad0786",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e08e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFrequencyWithoutBrands(message):\n",
    "    tweets = pd.read_csv('./tweets/allLabeledTweets.csv')\n",
    "\n",
    "    allPostsConcat = ''\n",
    "    for tweet in tweets[message]:\n",
    "        if(type(tweet)==str):\n",
    "            allPostsConcat+=' '+ tweet\n",
    "\n",
    "    # create bag-of-words\n",
    "    all_words = []\n",
    "\n",
    "    words = word_tokenize(allPostsConcat)\n",
    "    words = [word for word in words if not word in brands]\n",
    "    for word in words:\n",
    "        if word!='``' and word!=\"''\" and len(word) > 1:\n",
    "            all_words.append(word)\n",
    "\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b671a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find features in every post\n",
    "def find_features(post, tweet_features):\n",
    "    words = word_tokenize(post)\n",
    "    features = {}\n",
    "    for word in tweet_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f9a76",
   "metadata": {},
   "source": [
    "### Get training, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e05199fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((428, 25), (48, 25))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "train_df, test_df = model_selection.train_test_split(allLabeledTweets, test_size=0.1)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bab5fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_neutral = train_df[train_df[\"label\"] == 0]\n",
    "train_positive = train_df[train_df[\"label\"] == 1]\n",
    "train_negative = train_df[train_df[\"label\"] == 2]\n",
    "\n",
    "pd.DataFrame(dict(\n",
    "  neutral=[len(train_neutral)],\n",
    "  positive=[len(train_positive)],\n",
    "  negative=[len(train_negative)]  \n",
    ")).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "685101bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 25), (48, 25))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([\n",
    "    train_neutral.sample(75),\n",
    "    train_positive,\n",
    "    train_negative.sample(75)])\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf4150",
   "metadata": {},
   "source": [
    "### Lowercase raw tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533ea235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 4538\n",
      "Most common words: [('https', 476), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 79), ('ka', 73), ('uz', 49), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('pēc', 38), ('bet', 38), ('circleklatvija', 35), ('nav', 34), ('lieliskamūzika', 34), ('17-18', 34), ('►https', 34), ('//t.co/jymqnspivg', 34), ('skaties', 31), ('...', 29), ('to', 28), ('bitelv', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('maximaveikals', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('30', 14), ('gada', 13), ('var', 13), ('dblv', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('manslmt', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('lattelecom', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('hk', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('bankas', 8)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsLowercase = getWordFrequencyWithoutBrands('message_lowercase')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsLowercase)))\n",
    "print('Most common words: {}'.format(freqWordsLowercase.most_common(100)))\n",
    "\n",
    "word_features_lowercase = list(freqWordsLowercase.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "522ad1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"message_lowercase\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_lowercase), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"message_lowercase\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_lowercase), label) for (text, label) in tweets_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18435fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 58.333333333333336\n"
     ]
    }
   ],
   "source": [
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e3278b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.52      0.64        27\n",
      "           1       0.25      0.29      0.27         7\n",
      "           2       0.52      0.86      0.65        14\n",
      "\n",
      "    accuracy                           0.58        48\n",
      "   macro avg       0.53      0.55      0.52        48\n",
      "weighted avg       0.65      0.58      0.59        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         14        6        7\n",
       "       positive         1        2        4\n",
       "       negative         2        0       12"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dbd45",
   "metadata": {},
   "source": [
    "### Cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a23af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3862\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 79), ('ka', 73), ('uz', 49), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('pēc', 38), ('bet', 38), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('to', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('bez', 7)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsClean = getWordFrequencyWithoutBrands('clean_message')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsClean)))\n",
    "print('Most common words: {}'.format(freqWordsClean.most_common(100)))\n",
    "\n",
    "word_features_clean = list(freqWordsClean.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94d0fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68        27\n",
      "           1       0.25      0.29      0.27         7\n",
      "           2       0.52      0.86      0.65        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.55      0.57      0.53        48\n",
      "weighted avg       0.68      0.60      0.61        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         15        5        7\n",
       "       positive         1        2        4\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080aa8b",
   "metadata": {},
   "source": [
    "### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d10685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3834\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 80), ('ka', 73), ('uz', 50), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('vēl', 40), ('kopā', 40), ('latvijas', 39), ('pēc', 38), ('bet', 38), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('to', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunct = getWordFrequencyWithoutBrands('clean_message_no_punct')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunct)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunct.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct = list(freqWordsCleanNoPunct.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9cb97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68        27\n",
      "           1       0.29      0.29      0.29         7\n",
      "           2       0.50      0.86      0.63        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.56      0.57      0.53        48\n",
      "weighted avg       0.68      0.60      0.61        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         15        4        8\n",
       "       positive         1        2        4\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8a1a5",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3efe9",
   "metadata": {},
   "source": [
    "#### From list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49246458",
   "metadata": {},
   "source": [
    "##### With punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7a1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3787\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('to', 28), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('es', 22), ('tas', 22), ('šeit', 22), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('tagad', 18), ('man', 17), ('šodien', 17), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('izlase', 12), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('vietas', 9), ('varētu', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('..', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('eur', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoStopwordsFromList = getWordFrequencyWithoutBrands('clean_message_no_stopwords_from_list')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoStopwordsFromList)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoStopwordsFromList.most_common(100)))\n",
    "\n",
    "word_features_clean_no_stopwords_from_list = list(freqWordsCleanNoStopwordsFromList.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cb5da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75        27\n",
      "           1       0.20      0.29      0.24         7\n",
      "           2       0.53      0.64      0.58        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.53      0.53      0.52        48\n",
      "weighted avg       0.67      0.60      0.63        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        4        5\n",
       "       positive         2        2        3\n",
       "       negative         1        4        9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_stopwords_from_list\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_stopwords_from_list), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_stopwords_from_list\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_stopwords_from_list), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27df574",
   "metadata": {},
   "source": [
    "##### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427abf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3759\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('kas', 41), ('vēl', 40), ('kopā', 40), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('to', 28), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('es', 22), ('tas', 22), ('šeit', 22), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('tagad', 18), ('man', 17), ('šodien', 17), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('izlase', 12), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('vietas', 9), ('varētu', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunctNoStopwordsFromList = getWordFrequencyWithoutBrands('clean_message_no_punct_no_stopwords_from_list')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunctNoStopwordsFromList)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunctNoStopwordsFromList.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct_no_stopwords_from_list = list(freqWordsCleanNoPunctNoStopwordsFromList.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24318243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 62.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75        27\n",
      "           1       0.27      0.43      0.33         7\n",
      "           2       0.56      0.64      0.60        14\n",
      "\n",
      "    accuracy                           0.62        48\n",
      "   macro avg       0.56      0.58      0.56        48\n",
      "weighted avg       0.69      0.62      0.65        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        4        5\n",
       "       positive         2        3        2\n",
       "       negative         1        4        9"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct_no_stopwords_from_list\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct_no_stopwords_from_list), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct_no_stopwords_from_list\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct_no_stopwords_from_list), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87f27b",
   "metadata": {},
   "source": [
    "#### From most frequently used words excluding brand names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa2d9f",
   "metadata": {},
   "source": [
    "##### With punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ff1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3818\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('lielfans', 19), ('minūtēm', 19), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('oik', 15), ('līga', 15), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('vietas', 9), ('varētu', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('bez', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('esmu', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('..', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('tik', 6), ('eur', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('te', 6), ('būt', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6), ('šovakar', 6), ('binde', 6), ('lielveikala', 6), ('ltfa', 6), ('prizma', 6), ('daudz', 5), ('tieši', 5), ('pirms', 5)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoFreqStopwords = getWordFrequencyWithoutBrands('clean_message_no_freq_stopwords')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoFreqStopwords)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoFreqStopwords.most_common(100)))\n",
    "\n",
    "word_features_clean_no_freq_stopwords = list(freqWordsCleanNoFreqStopwords.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e810760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 64.58333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.73        27\n",
      "           1       0.30      0.43      0.35         7\n",
      "           2       0.62      0.71      0.67        14\n",
      "\n",
      "    accuracy                           0.65        48\n",
      "   macro avg       0.58      0.60      0.58        48\n",
      "weighted avg       0.69      0.65      0.66        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        5        4\n",
       "       positive         2        3        2\n",
       "       negative         2        2       10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_freq_stopwords\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_freq_stopwords), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_freq_stopwords\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_freq_stopwords), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16338568",
   "metadata": {},
   "source": [
    "##### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a50dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3790\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('lielfans', 19), ('minūtēm', 19), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('oik', 15), ('līga', 15), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('vietas', 9), ('varētu', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7), ('bez', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('esmu', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('tik', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('te', 6), ('būt', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6), ('šovakar', 6), ('binde', 6), ('lielveikala', 6), ('ltfa', 6), ('prizma', 6), ('daudz', 5), ('tieši', 5), ('pirms', 5), ('rokas', 5)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunctNoFreqStopwords = getWordFrequencyWithoutBrands('clean_message_no_punct_no_freq_stopwords')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunctNoFreqStopwords)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunctNoFreqStopwords.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct_no_freq_stopwords = list(freqWordsCleanNoPunctNoFreqStopwords.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa5da773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 70.83333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.78        27\n",
      "           1       0.38      0.43      0.40         7\n",
      "           2       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.71        48\n",
      "   macro avg       0.64      0.66      0.64        48\n",
      "weighted avg       0.73      0.71      0.71        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         19        4        4\n",
       "       positive         2        3        2\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct_no_freq_stopwords\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct_no_freq_stopwords), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct_no_freq_stopwords\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct_no_freq_stopwords), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ed9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cf338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d0967b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'neviens pilnīgi neviens drogas kasieris īpašais piedāvājums nē nopietni drogas vadība izdomāja kasieriem zombijiem jāpiedāvā īpašais piedāvājums necilvēcīgi redzu viņu dvēsele mirst izrunājot tos vārdus URL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-486ad3bd117c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#Predict the response for test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    766\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    767\u001b[0m         \"\"\"\n\u001b[1;32m--> 768\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'neviens pilnīgi neviens drogas kasieris īpašais piedāvājums nē nopietni drogas vadība izdomāja kasieriem zombijiem jāpiedāvā īpašais piedāvājums necilvēcīgi redzu viņu dvēsele mirst izrunājot tos vārdus URL'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = [(find_features(text, word_features_clean_no_punct_no_freq_stopwords), label) for (text, label) in allLabeledTweets]\n",
    "y = allLabeledTweets['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0147f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 230]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-c54db7474275>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cross-validation scores:{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    441\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \"\"\"\n\u001b[1;32m--> 231\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 230]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), [t], l, cv = 10, scoring='accuracy', error_score=\"raise\")\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67a78b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-aa785adca3fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a11d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
