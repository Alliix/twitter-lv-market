{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0bdec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aligo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8eeff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = [\n",
    "  \"latvenergo\",\n",
    "  \"rimi\",\n",
    "  \"maxima\",\n",
    "  \"swedbank\",\n",
    "  \"lmt\",\n",
    "  \"tet\",\n",
    "  \"lg\",\n",
    "  \"tele2\",\n",
    "  \"airbaltic\",\n",
    "  \"olympic\",\n",
    "  \"seb\",\n",
    "  \"grindeks\",\n",
    "  \"citadele\",\n",
    "  \"bite\",\n",
    "  \"drogas\",\n",
    "  \"depo\",\n",
    "  \"circlek\",\n",
    "  \"lb\",\n",
    "  \"optibet\",\n",
    "  \"evolution\",\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ade0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>language</th>\n",
       "      <th>inReplyToStatusId</th>\n",
       "      <th>inReplyToUserId</th>\n",
       "      <th>inReplyToScreenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>...</th>\n",
       "      <th>retweetedId</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>label</th>\n",
       "      <th>message_lowercase</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>clean_message_no_punct</th>\n",
       "      <th>clean_message_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_freq_stopwords</th>\n",
       "      <th>clean_message_no_freq_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>1478404</td>\n",
       "      <td>925617390523732000</td>\n",
       "      <td>2017-11-01T08:55:57</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24855060</td>\n",
       "      <td>Dienas Bizness</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>vērtēti trīs potenciālie airbaltic investori U...</td>\n",
       "      <td>vērtēti trīs potenciālie airbaltic investori U...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "      <td>tiek vērtēti trīs potenciālie airbaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>1486476</td>\n",
       "      <td>924948828318511100</td>\n",
       "      <td>2017-10-30T12:39:20</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44849531</td>\n",
       "      <td>RIGA I Airport</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien, vēlu vakarā, ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien  vēlu vakarā  ar svinīgu pasākumu ti...</td>\n",
       "      <td>vakardien , vēlu vakarā , svinīgu pasākumu atk...</td>\n",
       "      <td>vakardien vēlu vakarā svinīgu pasākumu atklāts...</td>\n",
       "      <td>vakardien vēlu vakarā svinīgu pasākumu tika at...</td>\n",
       "      <td>vakardien , vēlu vakarā , svinīgu pasākumu tik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Es ļoti ceru,ka potenciālie Air Baltic investo...</td>\n",
       "      <td>1488297</td>\n",
       "      <td>925794560420311000</td>\n",
       "      <td>2017-11-01T20:39:58</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218804015</td>\n",
       "      <td>Artis Pabriks</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>es ļoti ceru,ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru,ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru ka potenciālie air baltic investo...</td>\n",
       "      <td>es ļoti ceru , potenciālie air baltic investor...</td>\n",
       "      <td>es ļoti ceru potenciālie air baltic investori ...</td>\n",
       "      <td>ļoti ceru potenciālie air baltic investori nav...</td>\n",
       "      <td>ļoti ceru , potenciālie air baltic investori n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ar kritisko domāšanu gan joprojām bēdīgi. Piln...</td>\n",
       "      <td>1489818</td>\n",
       "      <td>925045141018218500</td>\n",
       "      <td>2017-10-30T19:02:02</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21071644</td>\n",
       "      <td>Andris Rubīns</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi. piln...</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi. piln...</td>\n",
       "      <td>ar kritisko domāšanu gan joprojām bēdīgi  piln...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi . pilns face...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi pilns facebo...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi pilns facebo...</td>\n",
       "      <td>kritisko domāšanu joprojām bēdīgi . pilns face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latvijas lidsabiedrība airBaltic sadarbībā ar ...</td>\n",
       "      <td>1490250</td>\n",
       "      <td>925006964135952400</td>\n",
       "      <td>2017-10-30T16:30:20</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131562098</td>\n",
       "      <td>Latviesi.com</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā ar ...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "      <td>latvijas lidsabiedrība airbaltic sadarbībā apv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message       id  \\\n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...  1478404   \n",
       "1  Vakardien, vēlu vakarā, ar svinīgu pasākumu ti...  1486476   \n",
       "2  Es ļoti ceru,ka potenciālie Air Baltic investo...  1488297   \n",
       "3  Ar kritisko domāšanu gan joprojām bēdīgi. Piln...  1489818   \n",
       "4  Latvijas lidsabiedrība airBaltic sadarbībā ar ...  1490250   \n",
       "\n",
       "              tweetId            createdAt language  inReplyToStatusId  \\\n",
       "0  925617390523732000  2017-11-01T08:55:57       lv                NaN   \n",
       "1  924948828318511100  2017-10-30T12:39:20       lv                NaN   \n",
       "2  925794560420311000  2017-11-01T20:39:58       lv                NaN   \n",
       "3  925045141018218500  2017-10-30T19:02:02       lv                NaN   \n",
       "4  925006964135952400  2017-10-30T16:30:20       lv                NaN   \n",
       "\n",
       "   inReplyToUserId  inReplyToScreenName     userId        userName  ...  \\\n",
       "0              NaN                  NaN   24855060  Dienas Bizness  ...   \n",
       "1              NaN                  NaN   44849531  RIGA I Airport  ...   \n",
       "2              NaN                  NaN  218804015   Artis Pabriks  ...   \n",
       "3              NaN                  NaN   21071644   Andris Rubīns  ...   \n",
       "4              NaN                  NaN  131562098    Latviesi.com  ...   \n",
       "\n",
       "  retweetedId  retweetCount label  \\\n",
       "0         NaN             0     0   \n",
       "1         NaN             0     0   \n",
       "2         NaN             9     2   \n",
       "3         NaN             3     2   \n",
       "4         NaN             0     0   \n",
       "\n",
       "                                   message_lowercase  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien, vēlu vakarā, ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru,ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi. piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                                       clean_message  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien, vēlu vakarā, ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru,ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi. piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                              clean_message_no_punct  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien  vēlu vakarā  ar svinīgu pasākumu ti...   \n",
       "2  es ļoti ceru ka potenciālie air baltic investo...   \n",
       "3  ar kritisko domāšanu gan joprojām bēdīgi  piln...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā ar ...   \n",
       "\n",
       "                clean_message_no_stopwords_from_list  \\\n",
       "0  vērtēti trīs potenciālie airbaltic investori U...   \n",
       "1  vakardien , vēlu vakarā , svinīgu pasākumu atk...   \n",
       "2  es ļoti ceru , potenciālie air baltic investor...   \n",
       "3  kritisko domāšanu joprojām bēdīgi . pilns face...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "       clean_message_no_punct_no_stopwords_from_list  \\\n",
       "0  vērtēti trīs potenciālie airbaltic investori U...   \n",
       "1  vakardien vēlu vakarā svinīgu pasākumu atklāts...   \n",
       "2  es ļoti ceru potenciālie air baltic investori ...   \n",
       "3  kritisko domāšanu joprojām bēdīgi pilns facebo...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "            clean_message_no_punct_no_freq_stopwords  \\\n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...   \n",
       "1  vakardien vēlu vakarā svinīgu pasākumu tika at...   \n",
       "2  ļoti ceru potenciālie air baltic investori nav...   \n",
       "3  kritisko domāšanu joprojām bēdīgi pilns facebo...   \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...   \n",
       "\n",
       "                     clean_message_no_freq_stopwords  \n",
       "0  tiek vērtēti trīs potenciālie airbaltic invest...  \n",
       "1  vakardien , vēlu vakarā , svinīgu pasākumu tik...  \n",
       "2  ļoti ceru , potenciālie air baltic investori n...  \n",
       "3  kritisko domāšanu joprojām bēdīgi . pilns face...  \n",
       "4  latvijas lidsabiedrība airbaltic sadarbībā apv...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLabeledTweets = pd.read_csv('./tweets/allLabeledTweets.csv')\n",
    "allLabeledTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849c310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    253\n",
       "2    136\n",
       "1     87\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLabeledTweets[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad0786",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e08e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFrequencyWithoutBrands(message):\n",
    "    tweets = pd.read_csv('./tweets/allLabeledTweets.csv')\n",
    "\n",
    "    allPostsConcat = ''\n",
    "    for tweet in tweets[message]:\n",
    "        if(type(tweet)==str):\n",
    "            allPostsConcat+=' '+ tweet\n",
    "\n",
    "    # create bag-of-words\n",
    "    all_words = []\n",
    "\n",
    "    words = word_tokenize(allPostsConcat)\n",
    "    words = [word for word in words if not word in brands]\n",
    "    for word in words:\n",
    "        if word!='``' and word!=\"''\" and len(word) > 1:\n",
    "            all_words.append(word)\n",
    "\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b671a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find features in every post\n",
    "def find_features(post, tweet_features):\n",
    "    words = word_tokenize(post)\n",
    "    features = {}\n",
    "    for word in tweet_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f9a76",
   "metadata": {},
   "source": [
    "### Get training, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e05199fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((428, 25), (48, 25))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "train_df, test_df = model_selection.train_test_split(allLabeledTweets, test_size=0.1)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bab5fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_neutral = train_df[train_df[\"label\"] == 0]\n",
    "train_positive = train_df[train_df[\"label\"] == 1]\n",
    "train_negative = train_df[train_df[\"label\"] == 2]\n",
    "\n",
    "pd.DataFrame(dict(\n",
    "  neutral=[len(train_neutral)],\n",
    "  positive=[len(train_positive)],\n",
    "  negative=[len(train_negative)]  \n",
    ")).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "685101bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 25), (48, 25))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([\n",
    "    train_neutral.sample(75),\n",
    "    train_positive,\n",
    "    train_negative.sample(75)])\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf4150",
   "metadata": {},
   "source": [
    "### Lowercase raw tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533ea235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 4538\n",
      "Most common words: [('https', 476), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 79), ('ka', 73), ('uz', 49), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('pēc', 38), ('bet', 38), ('circleklatvija', 35), ('nav', 34), ('lieliskamūzika', 34), ('17-18', 34), ('►https', 34), ('//t.co/jymqnspivg', 34), ('skaties', 31), ('...', 29), ('to', 28), ('bitelv', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('maximaveikals', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('30', 14), ('gada', 13), ('var', 13), ('dblv', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('manslmt', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('lattelecom', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('hk', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('bankas', 8)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsLowercase = getWordFrequencyWithoutBrands('message_lowercase')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsLowercase)))\n",
    "print('Most common words: {}'.format(freqWordsLowercase.most_common(100)))\n",
    "\n",
    "word_features_lowercase = list(freqWordsLowercase.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "522ad1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"message_lowercase\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_lowercase), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"message_lowercase\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_lowercase), label) for (text, label) in tweets_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18435fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 58.333333333333336\n"
     ]
    }
   ],
   "source": [
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e3278b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.52      0.64        27\n",
      "           1       0.25      0.29      0.27         7\n",
      "           2       0.52      0.86      0.65        14\n",
      "\n",
      "    accuracy                           0.58        48\n",
      "   macro avg       0.53      0.55      0.52        48\n",
      "weighted avg       0.65      0.58      0.59        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         14        6        7\n",
       "       positive         1        2        4\n",
       "       negative         2        0       12"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dbd45",
   "metadata": {},
   "source": [
    "### Cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a23af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3862\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 79), ('ka', 73), ('uz', 49), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('pēc', 38), ('bet', 38), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('to', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('bez', 7)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsClean = getWordFrequencyWithoutBrands('clean_message')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsClean)))\n",
    "print('Most common words: {}'.format(freqWordsClean.most_common(100)))\n",
    "\n",
    "word_features_clean = list(freqWordsClean.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94d0fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68        27\n",
      "           1       0.25      0.29      0.27         7\n",
      "           2       0.52      0.86      0.65        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.55      0.57      0.53        48\n",
      "weighted avg       0.68      0.60      0.61        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         15        5        7\n",
       "       positive         1        2        4\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080aa8b",
   "metadata": {},
   "source": [
    "### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d10685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3834\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('un', 191), ('ir', 118), ('ar', 117), ('par', 111), ('no', 80), ('ka', 73), ('uz', 50), ('jau', 49), ('vai', 48), ('arī', 43), ('kā', 42), ('kas', 41), ('vēl', 40), ('kopā', 40), ('latvijas', 39), ('pēc', 38), ('bet', 38), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('to', 28), ('tā', 26), ('tiešraide', 26), ('lhf', 25), ('pie', 24), ('kopāspēks', 24), ('spēli', 24), ('ja', 23), ('es', 22), ('tas', 22), ('tad', 22), ('šeit', 22), ('būs', 20), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('lai', 18), ('tagad', 18), ('man', 17), ('šodien', 17), ('tikai', 16), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('nekā', 12), ('gan', 11), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('bija', 10), ('vietas', 9), ('līdz', 9), ('nu', 9), ('varētu', 9), ('jo', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('pa', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunct = getWordFrequencyWithoutBrands('clean_message_no_punct')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunct)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunct.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct = list(freqWordsCleanNoPunct.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9cb97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68        27\n",
      "           1       0.29      0.29      0.29         7\n",
      "           2       0.50      0.86      0.63        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.56      0.57      0.53        48\n",
      "weighted avg       0.68      0.60      0.61        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         15        4        8\n",
       "       positive         1        2        4\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8a1a5",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3efe9",
   "metadata": {},
   "source": [
    "#### From list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49246458",
   "metadata": {},
   "source": [
    "##### With punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7a1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3787\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('kas', 41), ('kopā', 40), ('latvijas', 39), ('vēl', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('to', 28), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('es', 22), ('tas', 22), ('šeit', 22), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('tagad', 18), ('man', 17), ('šodien', 17), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('izlase', 12), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('vietas', 9), ('varētu', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('..', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('eur', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoStopwordsFromList = getWordFrequencyWithoutBrands('clean_message_no_stopwords_from_list')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoStopwordsFromList)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoStopwordsFromList.most_common(100)))\n",
    "\n",
    "word_features_clean_no_stopwords_from_list = list(freqWordsCleanNoStopwordsFromList.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cb5da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 60.416666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75        27\n",
      "           1       0.20      0.29      0.24         7\n",
      "           2       0.53      0.64      0.58        14\n",
      "\n",
      "    accuracy                           0.60        48\n",
      "   macro avg       0.53      0.53      0.52        48\n",
      "weighted avg       0.67      0.60      0.63        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        4        5\n",
       "       positive         2        2        3\n",
       "       negative         1        4        9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_stopwords_from_list\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_stopwords_from_list), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_stopwords_from_list\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_stopwords_from_list), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27df574",
   "metadata": {},
   "source": [
    "##### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427abf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3759\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('kas', 41), ('vēl', 40), ('kopā', 40), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('to', 28), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('es', 22), ('tas', 22), ('šeit', 22), ('the', 20), ('lielfans', 19), ('minūtēm', 19), ('tagad', 18), ('man', 17), ('šodien', 17), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('mūsu', 15), ('oik', 15), ('līga', 15), ('kad', 14), ('eiro', 14), ('gada', 13), ('izlase', 12), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('kur', 10), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('ko', 10), ('vietas', 9), ('varētu', 9), ('kāpēc', 9), ('kāds', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunctNoStopwordsFromList = getWordFrequencyWithoutBrands('clean_message_no_punct_no_stopwords_from_list')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunctNoStopwordsFromList)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunctNoStopwordsFromList.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct_no_stopwords_from_list = list(freqWordsCleanNoPunctNoStopwordsFromList.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24318243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 62.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75        27\n",
      "           1       0.27      0.43      0.33         7\n",
      "           2       0.56      0.64      0.60        14\n",
      "\n",
      "    accuracy                           0.62        48\n",
      "   macro avg       0.56      0.58      0.56        48\n",
      "weighted avg       0.69      0.62      0.65        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        4        5\n",
       "       positive         2        3        2\n",
       "       negative         1        4        9"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct_no_stopwords_from_list\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct_no_stopwords_from_list), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct_no_stopwords_from_list\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct_no_stopwords_from_list), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87f27b",
   "metadata": {},
   "source": [
    "#### From most frequently used words excluding brand names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa2d9f",
   "metadata": {},
   "source": [
    "##### With punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ff1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3818\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('...', 29), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('lielfans', 19), ('minūtēm', 19), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('oik', 15), ('līga', 15), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('vietas', 9), ('varētu', 9), ('valsts', 9), ('u-', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('darbu', 9), ('jaunajā', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('zālē', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('iespējams', 7), ('bez', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('esmu', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('..', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('tik', 6), ('eur', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('te', 6), ('būt', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6), ('šovakar', 6), ('binde', 6), ('lielveikala', 6), ('ltfa', 6), ('prizma', 6), ('daudz', 5), ('tieši', 5), ('pirms', 5)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoFreqStopwords = getWordFrequencyWithoutBrands('clean_message_no_freq_stopwords')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoFreqStopwords)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoFreqStopwords.most_common(100)))\n",
    "\n",
    "word_features_clean_no_freq_stopwords = list(freqWordsCleanNoFreqStopwords.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e810760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 64.58333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.73        27\n",
      "           1       0.30      0.43      0.35         7\n",
      "           2       0.62      0.71      0.67        14\n",
      "\n",
      "    accuracy                           0.65        48\n",
      "   macro avg       0.58      0.60      0.58        48\n",
      "weighted avg       0.69      0.65      0.66        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         18        5        4\n",
       "       positive         2        3        2\n",
       "       negative         2        2       10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_freq_stopwords\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_freq_stopwords), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_freq_stopwords\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_freq_stopwords), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16338568",
   "metadata": {},
   "source": [
    "##### No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a50dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3790\n",
      "Most common words: [('URL', 510), ('MENTION', 329), ('NMBR', 324), ('latvijas', 39), ('nav', 34), ('lieliskamūzika', 34), ('skaties', 31), ('tiešraide', 26), ('lhf', 25), ('kopāspēks', 24), ('spēli', 24), ('lielfans', 19), ('minūtēm', 19), ('hokeja', 16), ('ļoti', 15), ('rīgas', 15), ('oik', 15), ('līga', 15), ('eiro', 14), ('gada', 13), ('var', 13), ('ne', 12), ('izlase', 12), ('pret', 11), ('vairāk', 11), ('dienasbizness', 11), ('ielā', 11), ('tv', 11), ('darbinieku', 10), ('latvija', 10), ('amp', 10), ('vietas', 9), ('varētu', 9), ('valsts', 9), ('lauvassirds', 9), ('paldies', 9), ('rīgā', 9), ('telpu', 9), ('lv', 9), ('darbu', 9), ('jaunajā', 9), ('zālē', 9), ('virslīga', 9), ('nevar', 8), ('neko', 8), ('visu', 8), ('jums', 8), ('dblv', 8), ('bankas', 8), ('elektrības', 8), ('hk', 8), ('kāda', 7), ('interesanti', 7), ('eur', 7), ('SMILE', 7), ('iespējams', 7), ('bez', 7), ('tiešām', 7), ('dienu', 7), ('mums', 7), ('šī', 7), ('esmu', 7), ('cenas', 7), ('pērn', 7), ('mačā', 7), ('saņem', 7), ('latvijā', 7), ('elektrību', 7), ('savu', 7), ('šo', 7), ('diena', 7), ('apvienošanu', 7), ('futbolā', 7), ('tik', 6), ('klientiem', 6), ('labi', 6), ('reizi', 6), ('foto', 6), ('te', 6), ('būt', 6), ('billy', 6), ('valdes', 6), ('ūdens', 6), ('miljonus', 6), ('enerģētikas', 6), ('in', 6), ('būtu', 6), ('tec', 6), ('prezidents', 6), ('lattelecom', 6), ('ielas', 6), ('šovakar', 6), ('binde', 6), ('lielveikala', 6), ('ltfa', 6), ('prizma', 6), ('daudz', 5), ('tieši', 5), ('pirms', 5), ('rokas', 5)]\n"
     ]
    }
   ],
   "source": [
    "freqWordsCleanNoPunctNoFreqStopwords = getWordFrequencyWithoutBrands('clean_message_no_punct_no_freq_stopwords')\n",
    "\n",
    "# print the total number of words and the 100 most common words\n",
    "print('Number of words: {}'.format(len(freqWordsCleanNoPunctNoFreqStopwords)))\n",
    "print('Most common words: {}'.format(freqWordsCleanNoPunctNoFreqStopwords.most_common(100)))\n",
    "\n",
    "word_features_clean_no_punct_no_freq_stopwords = list(freqWordsCleanNoPunctNoFreqStopwords.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa5da773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 70.83333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.78        27\n",
      "           1       0.38      0.43      0.40         7\n",
      "           2       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.71        48\n",
      "   macro avg       0.64      0.66      0.64        48\n",
      "weighted avg       0.73      0.71      0.71        48\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral         19        4        4\n",
       "       positive         2        3        2\n",
       "       negative         1        1       12"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = list(zip(train_df.loc[:,\"clean_message_no_punct_no_freq_stopwords\"].values,train_df.loc[:,\"label\"].values))\n",
    "featuresets_train = [(find_features(text, word_features_clean_no_punct_no_freq_stopwords), label) for (text, label) in tweets_train]\n",
    "\n",
    "tweets_test = list(zip(test_df.loc[:,\"clean_message_no_punct_no_freq_stopwords\"].values,test_df.loc[:,\"label\"].values))\n",
    "featuresets_test = [(find_features(text, word_features_clean_no_punct_no_freq_stopwords), label) for (text, label) in tweets_test]\n",
    "\n",
    "nltk_model = SklearnClassifier(MultinomialNB())\n",
    "nltk_model.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"{} Accuracy: {}\".format(\"Naive Bayes\", accuracy))\n",
    "\n",
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_model.classify_many(txt_features)\n",
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "    columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ed9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cf338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a11d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
