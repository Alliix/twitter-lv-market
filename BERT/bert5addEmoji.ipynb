{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e42df2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>language</th>\n",
       "      <th>inReplyToStatusId</th>\n",
       "      <th>inReplyToUserId</th>\n",
       "      <th>inReplyToScreenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>...</th>\n",
       "      <th>retweetedId</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>label</th>\n",
       "      <th>message_lowercase</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>clean_message_no_punct</th>\n",
       "      <th>clean_message_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_freq_stopwords</th>\n",
       "      <th>clean_message_no_freq_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rodas sajūta, ka arī @airBaltic ir kaut kāds @...</td>\n",
       "      <td>0</td>\n",
       "      <td>1213615462581440500</td>\n",
       "      <td>2020-01-05T00:17:28+00:00</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62004316</td>\n",
       "      <td>Edgars Eglītis</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>rodas sajūta, ka arī @airbaltic ir kaut kāds @...</td>\n",
       "      <td>rodas sajūta, ka arī MENTION ir kaut kāds MENT...</td>\n",
       "      <td>rodas sajūta  ka arī MENTION ir kaut kāds MENT...</td>\n",
       "      <td>rodas sajūta , MENTION kāds MENTION kaktu kant...</td>\n",
       "      <td>rodas sajūta MENTION kāds MENTION kaktu kantor...</td>\n",
       "      <td>rodas sajūta MENTION kaut MENTION kaktu kantor...</td>\n",
       "      <td>rodas sajūta , MENTION kaut MENTION kaktu kant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amsterdama, @airBaltic  smukulītes 👍🏻 https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>1213740889476128800</td>\n",
       "      <td>2020-01-05T08:35:52+00:00</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213752948</td>\n",
       "      <td>Dairis Kuciks</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>amsterdama, @airbaltic  smukulītes 👍🏻 https://...</td>\n",
       "      <td>amsterdama, MENTION smukulītes 👍🏻 URL</td>\n",
       "      <td>amsterdama  MENTION smukulītes 👍🏻 URL</td>\n",
       "      <td>amsterdama , MENTION smukulītes 👍🏻 URL</td>\n",
       "      <td>amsterdama MENTION smukulītes 👍🏻 URL</td>\n",
       "      <td>amsterdama MENTION smukulītes 👍🏻 URL</td>\n",
       "      <td>amsterdama , MENTION smukulītes 👍🏻 URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kārtējo reizi... \\r\\n@airBaltic vakar raudošai...</td>\n",
       "      <td>0</td>\n",
       "      <td>1214174186069012500</td>\n",
       "      <td>2020-01-06T13:17:38+00:00</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1107743410646069200</td>\n",
       "      <td>Selma</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>kārtējo reizi... \\r\\n@airbaltic vakar raudošai...</td>\n",
       "      <td>kārtējo reizi... MENTION vakar raudošai sievie...</td>\n",
       "      <td>kārtējo reizi    MENTION vakar raudošai sievie...</td>\n",
       "      <td>kārtējo reizi ... MENTION vakar raudošai sievi...</td>\n",
       "      <td>kārtējo reizi MENTION vakar raudošai sievietei...</td>\n",
       "      <td>kārtējo reizi MENTION vakar raudošai sievietei...</td>\n",
       "      <td>kārtējo reizi ... MENTION vakar raudošai sievi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.05% airBaltic akciju pieder Latvijas valsti...</td>\n",
       "      <td>0</td>\n",
       "      <td>1214176732456075300</td>\n",
       "      <td>2020-01-06T13:27:45+00:00</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1107743410646069200</td>\n",
       "      <td>Selma</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80.05% airbaltic akciju pieder latvijas valsti...</td>\n",
       "      <td>NMBR % airbaltic akciju pieder latvijas valsti...</td>\n",
       "      <td>NMBR   airbaltic akciju pieder latvijas valsti...</td>\n",
       "      <td>NMBR % airbaltic akciju pieder latvijas valsti...</td>\n",
       "      <td>NMBR airbaltic akciju pieder latvijas valstij ...</td>\n",
       "      <td>NMBR airbaltic akciju pieder latvijas valstij ...</td>\n",
       "      <td>NMBR % airbaltic akciju pieder latvijas valsti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laba ideja lidojumiem ar @airBaltic https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>1214197047382941700</td>\n",
       "      <td>2020-01-06T14:48:29+00:00</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110783755</td>\n",
       "      <td>Inga Gorbunova</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>laba ideja lidojumiem ar @airbaltic https://t....</td>\n",
       "      <td>laba ideja lidojumiem ar MENTION URL</td>\n",
       "      <td>laba ideja lidojumiem ar MENTION URL</td>\n",
       "      <td>laba ideja lidojumiem MENTION URL</td>\n",
       "      <td>laba ideja lidojumiem MENTION URL</td>\n",
       "      <td>laba ideja lidojumiem MENTION URL</td>\n",
       "      <td>laba ideja lidojumiem MENTION URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  id              tweetId  \\\n",
       "0  Rodas sajūta, ka arī @airBaltic ir kaut kāds @...   0  1213615462581440500   \n",
       "1  Amsterdama, @airBaltic  smukulītes 👍🏻 https://...   0  1213740889476128800   \n",
       "2  Kārtējo reizi... \\r\\n@airBaltic vakar raudošai...   0  1214174186069012500   \n",
       "3  80.05% airBaltic akciju pieder Latvijas valsti...   0  1214176732456075300   \n",
       "4  Laba ideja lidojumiem ar @airBaltic https://t....   0  1214197047382941700   \n",
       "\n",
       "                   createdAt language  inReplyToStatusId  inReplyToUserId  \\\n",
       "0  2020-01-05T00:17:28+00:00       lv                NaN              NaN   \n",
       "1  2020-01-05T08:35:52+00:00       lv                NaN              NaN   \n",
       "2  2020-01-06T13:17:38+00:00       lv                NaN              NaN   \n",
       "3  2020-01-06T13:27:45+00:00       lv                NaN              NaN   \n",
       "4  2020-01-06T14:48:29+00:00       lv                NaN              NaN   \n",
       "\n",
       "   inReplyToScreenName               userId        userName  ... retweetedId  \\\n",
       "0                  NaN             62004316  Edgars Eglītis  ...         NaN   \n",
       "1                  NaN            213752948   Dairis Kuciks  ...         NaN   \n",
       "2                  NaN  1107743410646069200           Selma  ...         NaN   \n",
       "3                  NaN  1107743410646069200           Selma  ...         NaN   \n",
       "4                  NaN            110783755  Inga Gorbunova  ...         NaN   \n",
       "\n",
       "   retweetCount label                                  message_lowercase  \\\n",
       "0             0     2  rodas sajūta, ka arī @airbaltic ir kaut kāds @...   \n",
       "1             0     1  amsterdama, @airbaltic  smukulītes 👍🏻 https://...   \n",
       "2            73     2  kārtējo reizi... \\r\\n@airbaltic vakar raudošai...   \n",
       "3             2     2  80.05% airbaltic akciju pieder latvijas valsti...   \n",
       "4             4     0  laba ideja lidojumiem ar @airbaltic https://t....   \n",
       "\n",
       "                                       clean_message  \\\n",
       "0  rodas sajūta, ka arī MENTION ir kaut kāds MENT...   \n",
       "1              amsterdama, MENTION smukulītes 👍🏻 URL   \n",
       "2  kārtējo reizi... MENTION vakar raudošai sievie...   \n",
       "3  NMBR % airbaltic akciju pieder latvijas valsti...   \n",
       "4               laba ideja lidojumiem ar MENTION URL   \n",
       "\n",
       "                              clean_message_no_punct  \\\n",
       "0  rodas sajūta  ka arī MENTION ir kaut kāds MENT...   \n",
       "1              amsterdama  MENTION smukulītes 👍🏻 URL   \n",
       "2  kārtējo reizi    MENTION vakar raudošai sievie...   \n",
       "3  NMBR   airbaltic akciju pieder latvijas valsti...   \n",
       "4               laba ideja lidojumiem ar MENTION URL   \n",
       "\n",
       "                clean_message_no_stopwords_from_list  \\\n",
       "0  rodas sajūta , MENTION kāds MENTION kaktu kant...   \n",
       "1             amsterdama , MENTION smukulītes 👍🏻 URL   \n",
       "2  kārtējo reizi ... MENTION vakar raudošai sievi...   \n",
       "3  NMBR % airbaltic akciju pieder latvijas valsti...   \n",
       "4                  laba ideja lidojumiem MENTION URL   \n",
       "\n",
       "       clean_message_no_punct_no_stopwords_from_list  \\\n",
       "0  rodas sajūta MENTION kāds MENTION kaktu kantor...   \n",
       "1               amsterdama MENTION smukulītes 👍🏻 URL   \n",
       "2  kārtējo reizi MENTION vakar raudošai sievietei...   \n",
       "3  NMBR airbaltic akciju pieder latvijas valstij ...   \n",
       "4                  laba ideja lidojumiem MENTION URL   \n",
       "\n",
       "            clean_message_no_punct_no_freq_stopwords  \\\n",
       "0  rodas sajūta MENTION kaut MENTION kaktu kantor...   \n",
       "1               amsterdama MENTION smukulītes 👍🏻 URL   \n",
       "2  kārtējo reizi MENTION vakar raudošai sievietei...   \n",
       "3  NMBR airbaltic akciju pieder latvijas valstij ...   \n",
       "4                  laba ideja lidojumiem MENTION URL   \n",
       "\n",
       "                     clean_message_no_freq_stopwords  \n",
       "0  rodas sajūta , MENTION kaut MENTION kaktu kant...  \n",
       "1             amsterdama , MENTION smukulītes 👍🏻 URL  \n",
       "2  kārtējo reizi ... MENTION vakar raudošai sievi...  \n",
       "3  NMBR % airbaltic akciju pieder latvijas valsti...  \n",
       "4                  laba ideja lidojumiem MENTION URL  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "df = pd.read_csv('./../Naive_Bayes/tweets/allLabeledTweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d6c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>language</th>\n",
       "      <th>inReplyToStatusId</th>\n",
       "      <th>inReplyToUserId</th>\n",
       "      <th>inReplyToScreenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>...</th>\n",
       "      <th>placeType</th>\n",
       "      <th>retweetedId</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>message_lowercase</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>clean_message_no_punct</th>\n",
       "      <th>clean_message_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_stopwords_from_list</th>\n",
       "      <th>clean_message_no_punct_no_freq_stopwords</th>\n",
       "      <th>clean_message_no_freq_stopwords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 message   id  tweetId  createdAt  language  \\\n",
       "label data_type                                               \n",
       "0     train          205  205      205        205       205   \n",
       "      val             37   37       37         37        37   \n",
       "1     train           71   71       71         71        71   \n",
       "      val             13   13       13         13        13   \n",
       "2     train          103  103      103        103       103   \n",
       "      val             18   18       18         18        18   \n",
       "\n",
       "                 inReplyToStatusId  inReplyToUserId  inReplyToScreenName  \\\n",
       "label data_type                                                            \n",
       "0     train                      0                0                    0   \n",
       "      val                        0                0                    0   \n",
       "1     train                      0                0                    0   \n",
       "      val                        0                0                    0   \n",
       "2     train                      0                0                    0   \n",
       "      val                        0                0                    0   \n",
       "\n",
       "                 userId  userName  ...  placeType  retweetedId  retweetCount  \\\n",
       "label data_type                    ...                                         \n",
       "0     train         205       205  ...          9            0           205   \n",
       "      val            37        37  ...          0            0            37   \n",
       "1     train          71        71  ...          5            0            71   \n",
       "      val            13        13  ...          4            0            13   \n",
       "2     train         103       103  ...         10            0           103   \n",
       "      val            18        18  ...          0            0            18   \n",
       "\n",
       "                 message_lowercase  clean_message  clean_message_no_punct  \\\n",
       "label data_type                                                             \n",
       "0     train                    205            205                     205   \n",
       "      val                       37             37                      37   \n",
       "1     train                     71             71                      71   \n",
       "      val                       13             13                      13   \n",
       "2     train                    103            103                     103   \n",
       "      val                       18             18                      18   \n",
       "\n",
       "                 clean_message_no_stopwords_from_list  \\\n",
       "label data_type                                         \n",
       "0     train                                       205   \n",
       "      val                                          37   \n",
       "1     train                                        71   \n",
       "      val                                          13   \n",
       "2     train                                       103   \n",
       "      val                                          18   \n",
       "\n",
       "                 clean_message_no_punct_no_stopwords_from_list  \\\n",
       "label data_type                                                  \n",
       "0     train                                                205   \n",
       "      val                                                   37   \n",
       "1     train                                                 71   \n",
       "      val                                                   13   \n",
       "2     train                                                103   \n",
       "      val                                                   18   \n",
       "\n",
       "                 clean_message_no_punct_no_freq_stopwords  \\\n",
       "label data_type                                             \n",
       "0     train                                           205   \n",
       "      val                                              37   \n",
       "1     train                                            71   \n",
       "      val                                              13   \n",
       "2     train                                           103   \n",
       "      val                                              18   \n",
       "\n",
       "                 clean_message_no_freq_stopwords  \n",
       "label data_type                                   \n",
       "0     train                                  205  \n",
       "      val                                     37  \n",
       "1     train                                   71  \n",
       "      val                                     13  \n",
       "2     train                                  103  \n",
       "      val                                     18  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b722d9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    205\n",
       "2    103\n",
       "1     71\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_type=='train']['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169bc273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71\n",
       "2    71\n",
       "1    71\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = [df[df.data_type=='val'].message, df[df.data_type=='val'].label]\n",
    "\n",
    "df_train = [df[df.data_type=='train'].message, df[df.data_type=='train'].label]\n",
    "df_train = pd.concat(df_train, axis=1, keys=[\"message\", \"label\"])\n",
    "\n",
    "df_0 = df_train[df_train['label']==0]\n",
    "df_1 = df_train[df_train['label']==1]\n",
    "df_2 = df_train[df_train['label']==2]\n",
    "\n",
    "df_0_downsampled = df_0.sample(df_1.shape[0])\n",
    "df_2_downsampled = df_2.sample(df_1.shape[0])\n",
    "\n",
    "df_train = pd.concat([df_0_downsampled, df_2_downsampled, df_1])\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1da2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7730dfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['😀']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= 'word embeddings are vectors 😀'\n",
    "list_of_space_separated_pieces = text.strip().split()\n",
    "ids = [tokenizer(piece, add_special_tokens=False)[\"input_ids\"] for piece in list_of_space_separated_pieces]\n",
    "unk_indices = [i for i, encoded in enumerate(ids) if tokenizer.unk_token_id in encoded]\n",
    "unknown_strings = [piece for i, piece in enumerate(list_of_space_separated_pieces) if i in unk_indices]\n",
    "unknown_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24ba76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6220634f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['👍🏻',\n",
       " '😊',\n",
       " '😏',\n",
       " 'vietā👍',\n",
       " '🇫🇷un',\n",
       " '🇱🇻',\n",
       " 'krāšņumā…',\n",
       " '🙂',\n",
       " '😠',\n",
       " '😏',\n",
       " '👏',\n",
       " 'n’',\n",
       " '🙈',\n",
       " 'oik…',\n",
       " '🎤🤖🚀',\n",
       " '🥂🍾',\n",
       " '👍',\n",
       " '🇷🇺🤥',\n",
       " '🇱🇻',\n",
       " '😡',\n",
       " '⚽️✅',\n",
       " 'rimi🇱🇻',\n",
       " 'rimi🇱🇹',\n",
       " '😊',\n",
       " '🍊',\n",
       " '🙄',\n",
       " '🤬',\n",
       " '😁',\n",
       " '👍',\n",
       " '🏁',\n",
       " '🤷🏻\\u200d♂️',\n",
       " '🥺un',\n",
       " '📲',\n",
       " '📲naudas',\n",
       " '📲',\n",
       " '…',\n",
       " '🎥',\n",
       " '👉',\n",
       " '⤵️',\n",
       " '🤔',\n",
       " '👍',\n",
       " '😄👍',\n",
       " 'zeltu😎😜😎=tāāā',\n",
       " '🎄būs',\n",
       " '❤️',\n",
       " 'bet…',\n",
       " 'bet…',\n",
       " 'bet…',\n",
       " '😭',\n",
       " '🤔',\n",
       " '🤕']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_tokens = []\n",
    "for message in df.clean_message_no_punct.values:\n",
    "    list_of_space_separated_pieces = message.strip().split()\n",
    "    ids = [tokenizer(piece, add_special_tokens=False)[\"input_ids\"] for piece in list_of_space_separated_pieces]\n",
    "    unk_indices = [i for i, encoded in enumerate(ids) if tokenizer.unk_token_id in encoded]\n",
    "    unknown_strings = [piece for i, piece in enumerate(list_of_space_separated_pieces) if i in unk_indices]\n",
    "    for unk_str in unknown_strings:\n",
    "        unk_tokens.append(unk_str)\n",
    "\n",
    "unk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15fb88f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('👍', 3), ('bet…', 3), ('😊', 2), ('😏', 2), ('🇱🇻', 2), ('📲', 2), ('🤔', 2), ('👍🏻', 1), ('vietā👍', 1), ('🇫🇷un', 1)]\n",
      "['👍', 'bet…', '😊', '😏', '🇱🇻', '📲', '🤔', '👍🏻', 'vietā👍', '🇫🇷un']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(unk_tokens)\n",
    "\n",
    "# print(counter)\n",
    "# print(counter.values())\n",
    "# print(counter.keys())\n",
    "print(counter.most_common(10))\n",
    "# print(dict(counter))\n",
    "most_common_values= [word for word, word_count in counter.most_common(10)]\n",
    "print(most_common_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66942668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de3478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23e40df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train.message.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].message.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f828f720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e837e5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(119557, 768)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e7cfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f10c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25934623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict = {0: 0, 1: 1, 2: 2,}\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c8d5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(torch.device('cpu')) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a6299a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782153ebb3dd44608a1eb0b1a3c14cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.0338713518330749\n",
      "Validation loss: 0.9223351685897164\n",
      "F1 Score (Weighted): 0.6591267586929417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.8085563270978524\n",
      "Validation loss: 0.7785827906235404\n",
      "F1 Score (Weighted): 0.6844688960471892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.6320491975671808\n",
      "Validation loss: 0.8700829196883284\n",
      "F1 Score (Weighted): 0.6226092792656259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d21cf2b0604b41b74dbd50f6c34f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14636/1693375889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                  }       \n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1530\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 )\n\u001b[0;32m    582\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     ):\n\u001b[1;32m--> 400\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DEV\\twitter-lv-market\\venv38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(torch.device('cpu')) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'models5/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa83edb",
   "metadata": {},
   "source": [
    "# Find emoji with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa5ca8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embeddings are vectors 😀\n",
      "['😀']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "emoji_str = \"😀 😃 😄 😁 😆 😅 😂 🤣 😊 😇 🙂 🙃 😉 😌 😍 🥰 😘 😗 😙 😚 😋 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏 😒 😞 😔 😟 😕 🙁 ☹️ 😣 😖 😫 😩 🥺 😢 😭 😤 😠 😡 🤬 🤯 😳 🥵 🥶 😱 😨 😰 😥 😓 🤗 🤔 🤭 🤫 🤥 😶 😐 😑 😬 🙄 😯 😦 😧 😮 😲 🥱 😴 🤤 😪 😵 🤐 🥴 🤢 🤮 🤧 😷 🤒 🤕 😺 😸 😹 😻 😼 😽 🙀 😿 😾 💩 👋 🤚 🖐 ✋ 🖖 👌 🤏 ✌️ 🤞 🤟 🤘 🤙 👈 👉 👆 🖕 👇 ☝️ 👍 👎 ✊ 👊 🤛 🤜 👏 🙌 👐 🤲 🤝 🙏'👋🏻 🤚🏻 🖐🏻 ✋🏻 🖖🏻 👌🏻 🤏🏻 ✌🏻 🤞🏻 🤟🏻 🤘🏻 🤙🏻 👈🏻 👉🏻 👆🏻 🖕🏻 👇🏻 ☝🏻 👍🏻 👎🏻 ✊🏻 👊🏻 🤛🏻 🤜🏻 👏🏻 🙌🏻 👐🏻 🤲🏻 🙏🏻 👋🏼 🤚🏼 🖐🏼 ✋🏼 🖖🏼 👌🏼 🤏🏼 ✌🏼 🤞🏼 🤟🏼 🤘🏼 🤙🏼 👈🏼 👉🏼 👆🏼 🖕🏼 👇🏼 ☝🏼 👍🏼 👎🏼 ✊🏼 👊🏼 🤛🏼 🤜🏼 👏🏼 🙌🏼 👐🏼 🤲🏼 🙏🏼 👋🏽 🤚🏽 🖐🏽 ✋🏽 🖖🏽 👌🏽 🤏🏽 ✌🏽 🤞🏽 🤟🏽 🤘🏽 🤙🏽 👈🏽 👉🏽 👆🏽 🖕🏽 👇🏽 ☝🏽 👍🏽 👎🏽 ✊🏽 👊🏽 🤛🏽 🤜🏽 👏🏽 🙌🏽 👐🏽 🤲🏽 🙏👋🏾 🤚🏾 🖐🏾 ✋🏾 🖖🏾 👌🏾 🤏🏾 ✌🏾 🤞🏾 🤟🏾 🤘🏾 🤙🏾 👈🏾 👉🏾 👆🏾 🖕🏾 👇🏾 ☝🏾 👍🏾 👎🏾 ✊🏾 👊🏾 🤛🏾 🤜🏾 👏🏾 🙌🏾 👐🏾 🤲🏾 🙏 👋🏿 🤚🏿 🖐🏿 ✋🏿 🖖🏿 👌🏿 🤏🏿 ✌🏿 🤞🏿 🤟🏿 🤘🏿 🤙🏿 👈🏿 👉🏿 👆🏿 🖕🏿 👇🏿 ☝🏿 👍🏿 👎🏿 ✊🏿 👊🏿 🤛🏿 🤜🏿 👏🏿 🙌🏿 👐🏿 🤲🏿 🙏🏿 ❤️ 🧡 💛 💚 💙 💜 🖤 🤍 🤎 💔 ❣️ 💕 💞 💓 💗 💖 💘 💝 💟 💑🏻 💑🏼 💑🏽 💑🏾 💑🏿 💏🏻 💏🏼 💏🏽 💏🏾 💏🏿 👨🏻‍❤️‍👨🏻 👨🏻‍❤️‍👨🏼 👨🏻‍❤️‍👨🏽 👨🏻‍❤️‍👨🏾 👨🏻‍❤️‍👨🏿 👨🏼‍❤️‍👨🏻 👨🏼‍❤️‍👨🏼 👨🏼‍❤️‍👨🏽 👨🏼‍❤️‍👨🏾 👨🏼‍❤️‍👨🏿 👨🏽‍❤️‍👨🏻 👨🏽‍❤️‍👨🏼 👨🏽‍❤️‍👨🏽 👨🏽‍❤️‍👨🏾 👨🏽‍❤️‍👨🏿 👨🏾‍❤️‍👨🏻 👨🏾‍❤️‍👨🏼 👨🏾‍❤️‍👨🏽 👨🏾‍❤️‍👨🏾 👨🏾‍❤️‍👨🏿 👨🏿‍❤️‍👨🏻 👨🏿‍❤️‍👨🏼 👨🏿‍❤️‍👨🏽 👨🏿‍❤️‍👨🏾 👨🏿‍❤️‍👨🏿 👩🏻‍❤️‍👨🏻 👩🏻‍❤️‍👨🏼 👩🏻‍❤️‍👨🏽 👩🏻‍❤️‍👨🏾 👩🏻‍❤️‍👨🏿 👩🏻‍❤️‍👩🏻 👩🏻‍❤️‍👩🏼 👩🏻‍❤️‍👩🏽 👩🏻‍❤️‍👩🏾 👩🏻‍❤️‍👩🏿 👩🏼‍❤️‍👨🏻 👩🏼‍❤️‍👨🏼 👩🏼‍❤️‍👨🏽 👩🏼‍❤️‍👨🏾 👩🏼‍❤️‍👨🏿 👩🏼‍❤️‍👩🏻 👩🏼‍❤️‍👩🏼 👩🏼‍❤️‍👩🏽 👩🏼‍❤️‍👩🏾 👩🏼‍❤️‍👩🏿 👩🏽‍❤️‍👨🏻 👩🏽‍❤️‍👨🏼 👩🏽‍❤️‍👨🏽 👩🏽‍❤️‍👨🏾 👩🏽‍❤️‍👨🏿 👩🏽‍❤️‍👩🏻 👩🏽‍❤️‍👩🏼 👩🏽‍❤️‍👩🏽 👩🏽‍❤️‍👩🏾 👩🏽‍❤️‍👩🏿 👩🏾‍❤️‍👨🏻 👩🏾‍❤️‍👨🏼 👩🏾‍❤️‍👨🏽 👩🏾‍❤️‍👨🏾 👩🏾‍❤️‍👨🏿 👩🏾‍❤️‍👩🏻 👩🏾‍❤️‍👩🏼 👩🏾‍❤️‍👩🏽 👩🏾‍❤️‍👩🏾 👩🏾‍❤️‍👩🏿 👩🏿‍❤️‍👨🏻 👩🏿‍❤️‍👨🏼 👩🏿‍❤️‍👨🏽 👩🏿‍❤️‍👨🏾 👩🏿‍❤️‍👨🏿 👩🏿‍❤️‍👩🏻 👩🏿‍❤️‍👩🏼 👩🏿‍❤️‍👩🏽 👩🏿‍❤️‍👩🏾 👩🏿‍❤️‍👩🏿 🧑🏻‍❤️‍🧑🏼 🧑🏻‍❤️‍🧑🏽 🧑🏻‍❤️‍🧑🏾 🧑🏻‍❤️‍🧑🏿 🧑🏼‍❤️‍🧑🏻 🧑🏼‍❤️‍🧑🏽 🧑🏼‍❤️‍🧑🏾 🧑🏼‍❤️‍🧑🏿 🧑🏽‍❤️‍🧑🏻 🧑🏽‍❤️‍🧑🏼 🧑🏽‍❤️‍🧑🏾 🧑🏽‍❤️‍🧑🏿 🧑🏾‍❤️‍🧑🏻 🧑🏾‍❤️‍🧑🏼 🧑🏾‍❤️‍🧑🏽 🧑🏾‍❤️‍🧑🏿 🧑🏿‍❤️‍🧑🏻 🧑🏿‍❤️‍🧑🏼 🧑🏿‍❤️‍🧑🏽 🧑🏿‍❤️‍🧑🏾 👨🏻‍❤️‍💋‍👨🏻 👨🏻‍❤️‍💋‍👨🏼 👨🏻‍❤️‍💋‍👨🏽 👨🏻‍❤️‍💋‍👨🏾 👨🏻‍❤️‍💋‍👨🏿 👨🏼‍❤️‍💋‍👨🏻 👨🏼‍❤️‍💋‍👨🏼 👨🏼‍❤️‍💋‍👨🏽 👨🏼‍❤️‍💋‍👨🏾 👨🏼‍❤️‍💋‍👨🏿 👨🏽‍❤️‍💋‍👨🏻 👨🏽‍❤️‍💋‍👨🏼 👨🏽‍❤️‍💋‍👨🏽 👨🏽‍❤️‍💋‍👨🏾 👨🏽‍❤️‍💋‍👨🏿 👨🏾‍❤️‍💋‍👨🏻 👨🏾‍❤️‍💋‍👨🏼 👨🏾‍❤️‍💋‍👨🏽 👨🏾‍❤️‍💋‍👨🏾 👨🏾‍❤️‍💋‍👨🏿 👨🏿‍❤️‍💋‍👨🏻 👨🏿‍❤️‍💋‍👨🏼 👨🏿‍❤️‍💋‍👨🏽 👨🏿‍❤️‍💋‍👨🏾 👨🏿‍❤️‍💋‍👨🏿 👩🏻‍❤️‍💋‍👨🏻 👩🏻‍❤️‍💋‍👨🏼 👩🏻‍❤️‍💋‍👨🏽 👩🏻‍❤️‍💋‍👨🏾 👩🏻‍❤️‍💋‍👨🏿 👩🏻‍❤️‍💋‍👩🏻 👩🏻‍❤️‍💋‍👩🏼 👩🏻‍❤️‍💋‍👩🏽 👩🏻‍❤️‍💋‍👩🏾 👩🏻‍❤️‍💋‍👩🏿 👩🏼‍❤️‍💋‍👨🏻 👩🏼‍❤️‍💋‍👨🏼 👩🏼‍❤️‍💋‍👨🏽 👩🏼‍❤️‍💋‍👨🏾 👩🏼‍❤️‍💋‍👨🏿 👩🏼‍❤️‍💋‍👩🏻 👩🏼‍❤️‍💋‍👩🏼 👩🏼‍❤️‍💋‍👩🏽 👩🏼‍❤️‍💋‍👩🏾 👩🏼‍❤️‍💋‍👩🏿 👩🏽‍❤️‍💋‍👨🏻 👩🏽‍❤️‍💋‍👨🏼 👩🏽‍❤️‍💋‍👨🏽 👩🏽‍❤️‍💋‍👨🏾 👩🏽‍❤️‍💋‍👨🏿 👩🏽‍❤️‍💋‍👩🏻 👩🏽‍❤️‍💋‍👩🏼 👩🏽‍❤️‍💋‍👩🏽 👩🏽‍❤️‍💋‍👩🏾 👩🏽‍❤️‍💋‍👩🏿 👩🏾‍❤️‍💋‍👨🏻 👩🏾‍❤️‍💋‍👨🏼 👩🏾‍❤️‍💋‍👨🏽 👩🏾‍❤️‍💋‍👨🏾 👩🏾‍❤️‍💋‍👨🏿 👩🏾‍❤️‍💋‍👩🏻 👩🏾‍❤️‍💋‍👩🏼 👩🏾‍❤️‍💋‍👩🏽 👩🏾‍❤️‍💋‍👩🏾 👩🏾‍❤️‍💋‍👩🏿 👩🏿‍❤️‍💋‍👨🏻 👩🏿‍❤️‍💋‍👨🏼 👩🏿‍❤️‍💋‍👨🏽 👩🏿‍❤️‍💋‍👨🏾 👩🏿‍❤️‍💋‍👨🏿 👩🏿‍❤️‍💋‍👩🏻 👩🏿‍❤️‍💋‍👩🏼 👩🏿‍❤️‍💋‍👩🏽 👩🏿‍❤️‍💋‍👩🏾 👩🏿‍❤️‍💋‍👩🏿 🧑🏻‍❤️‍💋‍🧑🏼 🧑🏻‍❤️‍💋‍🧑🏽 🧑🏻‍❤️‍💋‍🧑🏾 🧑🏻‍❤️‍💋‍🧑🏿 🧑🏼‍❤️‍💋‍🧑🏻 🧑🏼‍❤️‍💋‍🧑🏽 🧑🏼‍❤️‍💋‍🧑🏾 🧑🏼‍❤️‍💋‍🧑🏿 🧑🏽‍❤️‍💋‍🧑🏻 🧑🏽‍❤️‍💋‍🧑🏼 🧑🏽‍❤️‍💋‍🧑🏾 🧑🏽‍❤️‍💋‍🧑🏿 🧑🏾‍❤️‍💋‍🧑🏻 🧑🏾‍❤️‍💋‍🧑🏼 🧑🏾‍❤️‍💋‍🧑🏽 🧑🏾‍❤️‍💋‍🧑🏿 🧑🏿‍❤️‍💋‍🧑🏻 🧑🏿‍❤️‍💋‍🧑🏼 🧑🏿‍❤️‍💋‍🧑🏽 🧑🏿‍❤️‍💋‍🧑🏾 👭 🧑‍🤝‍🧑 👬 👫 👩‍❤️‍👩 💑 👨‍❤️‍👨 👩‍❤️‍👨 👩‍❤️‍💋‍👩 💏 👨‍❤️‍💋‍👨 👩‍❤️‍💋‍👨 👪 👨‍👩‍👦 👨‍👩‍👧 👨‍👩‍👧‍👦 👨‍👩‍👦‍👦 👨‍👩‍👧‍👧 👨‍👨‍👦 👨‍👨‍👧 👨‍👨‍👧‍👦 👨‍👨‍👦‍👦 👨‍👨‍👧‍👧 👩‍👩‍👦 👩‍👩‍👧 👩‍👩‍👧‍👦 👩‍👩‍👦‍👦 👩‍👩‍👧‍👧 👨‍👦 👨‍👦‍👦 👨‍👧 👨‍👧‍👦 👨‍👧‍👧 👩‍👦 👩‍👦‍👦 👩‍👧 👩‍👧‍👦 👩‍ 👧‍👧 💋\"\n",
    "emoji_list = emoji_str.split(' ')\n",
    "emoji_regex = '|'.join(emoji_list)\n",
    "foundEmoji = re.findall(emoji_regex,text)\n",
    "print(text)\n",
    "print(foundEmoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "faa4a083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "all_emoji = []\n",
    "for message in df.message:\n",
    "    foundEmoji = re.findall(emoji_regex, message)\n",
    "    for emoji in foundEmoji:\n",
    "        all_emoji.append(emoji)\n",
    "\n",
    "print(len(all_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3f792b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('👍', 6), ('😊', 2), ('😏', 2), ('🤔', 2), ('😎', 2), ('🙂', 1), ('😠', 1), ('👏', 1), ('🤥', 1), ('😡', 1), ('🙄', 1), ('🤬', 1), ('😁', 1), ('🥺', 1), ('👉', 1)]\n",
      "['👍', '😊', '😏', '🤔', '😎', '🙂', '😠', '👏', '🤥', '😡', '🙄', '🤬', '😁', '🥺', '👉']\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(all_emoji)\n",
    "\n",
    "# print(counter)\n",
    "# print(counter.values())\n",
    "# print(counter.keys())\n",
    "print(counter.most_common(15))\n",
    "# print(dict(counter))\n",
    "most_common_emoji= [word for word, word_count in counter.most_common(15)]\n",
    "print(most_common_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a86708f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119557, 768)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "tokenizer.add_tokens(most_common_values, special_tokens=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fcf9093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac1a6b54064f85bacaf227aea18a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.110438262912589\n",
      "Validation loss: 1.050589672897173\n",
      "F1 Score (Weighted): 0.4947186700767264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.1089339944678294\n",
      "Validation loss: 1.050589672897173\n",
      "F1 Score (Weighted): 0.4947186700767264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 1.1244843887611173\n",
      "Validation loss: 1.050589672897173\n",
      "F1 Score (Weighted): 0.4947186700767264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 1.1054569835394201\n",
      "Validation loss: 1.050589672897173\n",
      "F1 Score (Weighted): 0.4947186700767264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 1.1107762707790858\n",
      "Validation loss: 1.050589672897173\n",
      "F1 Score (Weighted): 0.4947186700767264\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(torch.device('cpu')) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'models5/finetuned_BERT_emoji_regex_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99f373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "venv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
